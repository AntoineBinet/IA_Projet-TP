{"cells":[{"cell_type":"markdown","source":"## Introduction à la quantization \n\nLaurent cetinsoy\n\nLes réseaux de neurones prennent beaucoup de place et il peut être difficile de les faire rentrer sur certains dispositifs embarqués. \n\nIl existe plusieurs méthodes pour réduire la taille et augmenter la vitesse d'executer des réseaux de neurone. Par exemple il y a ce qu'on appelle la quantization et le pruning.\n\nDans ce notebook on va faire une introduction à la quantization avec la librairie tensorflow lite.\n\n\n## Quantization post training\n\nDans un premier temps on va quantifier notre réseau après l'avoir entraîné normalement. \n\n\nEntraîner un réseau de neurone convolutionnel simple avec keras pour faire de la classification MNIST (ou un autre dataset simple de votre choix si (vous en avez marre de ce dataset - https://keras.io/api/datasets/)\n\n\n","metadata":{"tags":[],"cell_id":"b26fd776bc224d9c94f0502912a1082c","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\nfrom tensorflow.keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\ninput_shape = x_train.shape[1] * x_train.shape[2]\nx_train_flat = x_train.reshape(-1, input_shape)\nx_test_flat = x_test.reshape(-1, input_shape)\n\nx_train_normalized = x_train_flat / 255.0\nx_test_normalized = x_test_flat / 255.0\n\nmodel_relu = Sequential([\n    Dense(300, activation='relu', input_shape=(input_shape,)),\n    Dense(100, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\nmodel_relu.compile(optimizer=SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory_relu = model_relu.fit(x_train_normalized, y_train, epochs=10, validation_data=(x_test_normalized, y_test))\n","metadata":{"tags":[],"cell_id":"33ab9eae6bcc434bbebfde9369dbfc9a","source_hash":"a373533a","execution_start":1679833584338,"execution_millis":97228,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-03-26 12:26:25.286101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-26 12:26:25.451530: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-26 12:26:25.451565: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-26 12:26:25.490407: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-26 12:26:27.084041: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-03-26 12:26:27.084133: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-03-26 12:26:27.084143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-03-26 12:26:29.845427: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-03-26 12:26:29.845467: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-03-26 12:26:29.845488: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-3cbfad71-571a-4cfb-8f30-25f77afca130): /proc/driver/nvidia/version does not exist\n2023-03-26 12:26:29.845743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nEpoch 1/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.5820 - accuracy: 0.8491 - val_loss: 0.3020 - val_accuracy: 0.9148\nEpoch 2/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.2789 - accuracy: 0.9205 - val_loss: 0.2349 - val_accuracy: 0.9340\nEpoch 3/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.2259 - accuracy: 0.9351 - val_loss: 0.2031 - val_accuracy: 0.9414\nEpoch 4/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.1901 - accuracy: 0.9455 - val_loss: 0.1727 - val_accuracy: 0.9508\nEpoch 5/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.1640 - accuracy: 0.9533 - val_loss: 0.1560 - val_accuracy: 0.9563\nEpoch 6/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.1443 - accuracy: 0.9582 - val_loss: 0.1379 - val_accuracy: 0.9600\nEpoch 7/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.1282 - accuracy: 0.9635 - val_loss: 0.1268 - val_accuracy: 0.9636\nEpoch 8/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.1153 - accuracy: 0.9670 - val_loss: 0.1182 - val_accuracy: 0.9645\nEpoch 9/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.1045 - accuracy: 0.9702 - val_loss: 0.1098 - val_accuracy: 0.9676\nEpoch 10/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0951 - accuracy: 0.9725 - val_loss: 0.1042 - val_accuracy: 0.9696\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Afficher le nombre de paramètre du modèle","metadata":{"tags":[],"cell_id":"7606299f55c34e059a9d74783815d8b6","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"model_relu.summary()","metadata":{"tags":[],"cell_id":"8941c31031a94c048615663c817f06b7","source_hash":"cdca5fa","execution_start":1679833681567,"execution_millis":94,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 300)               235500    \n                                                                 \n dense_1 (Dense)             (None, 100)               30100     \n                                                                 \n dense_2 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Sauvegarder votre modèle et afficher la taille du fichier. Si on applique une bête règle de trois, quelle est la taille occupée par paramètre ? ","metadata":{"tags":[],"cell_id":"fcf42df8c41f4d3f8261bc1c1ba81333","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import os\n\nmodel_relu.save('model_relu.h5')\n\n\nfile_size = os.path.getsize('model_relu.h5')\ntotal_params = model_relu.count_params()\nsize_per_param = file_size / total_params\n\nprint(f\"taille occupée par paramètre: {size_per_param} octets\")","metadata":{"tags":[],"cell_id":"5863f3ac76dd4f02983769d2176b5cdf","source_hash":"24659467","execution_start":1679833681657,"execution_millis":41,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"taille occupée par paramètre: 4.070815048197742 octets\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"On va maintenant convertir notre modèle keras en modèle tensorflow lite. \n\nInstaller la librairie tensorflow lite créer une instance de la class TFLiteConverter à partir de votre modèle keras\n","metadata":{"tags":[],"cell_id":"d8edebc126fc47ae9e07686ee939fcb4","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import tensorflow as tf\n\nmodel = tf.keras.models.load_model('model_relu.h5')\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)","metadata":{"tags":[],"cell_id":"f015b091f62543919d513fa397dfc56d","source_hash":"eed079cf","execution_start":1679833709672,"execution_millis":47,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Convertir votre modèle et le sauvegarder dans un fichier nommé model.tflite. Sa taille est-elle plus petite ? ","metadata":{"tags":[],"cell_id":"84bbaf55586b486fa355cd83e1c6f5eb","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"tflite_model = converter.convert()\n\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\nkeras_file_size = os.path.getsize('model_relu.h5')\nprint(f\"taille  Keras: {keras_file_size} octets\")\n\ntflite_file_size = os.path.getsize('model.tflite')\nprint(f\"taille  TensorFlow Lite: {tflite_file_size} octets\")\n","metadata":{"tags":[],"cell_id":"ea7e21a18c33437db90d8fe5366f1a43","source_hash":"1714ac6a","execution_start":1679833810312,"execution_millis":1390,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"INFO:tensorflow:Assets written to: /tmp/tmp_nlsdhve/assets\ntaille  Keras: 1085320 octets\ntaille  TensorFlow Lite: 1068532 octets\n2023-03-26 12:30:11.417964: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2023-03-26 12:30:11.418009: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2023-03-26 12:30:11.418811: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp_nlsdhve\n2023-03-26 12:30:11.420130: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2023-03-26 12:30:11.420164: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp_nlsdhve\n2023-03-26 12:30:11.424484: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n2023-03-26 12:30:11.425414: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2023-03-26 12:30:11.464552: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp_nlsdhve\n2023-03-26 12:30:11.472195: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 53391 microseconds.\n2023-03-26 12:30:11.503287: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"On va maintenant spécifier des optimisations au converter. \n\n1. Recréer un converter\n\n2. modifier son attribut optimizations pour ajouter une liste d'optimisation avec la valeur tf.lite.Optimize.DEFAULT\n\n3. Relancer la conversion du modèle, sauvegarder le modèle et regarder la taille du fichier généré","metadata":{"tags":[],"cell_id":"c0cc780509734477a520227ff0d19230","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"#Recréer un converter\noptimized_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n#2modifier son attribut optimizations pour ajouter une liste d'optimisation avec la valeur tf.lite.Optimize.DEFAULT\noptimized_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n#3Relancer la conversion du modèle, sauvegarder le modèle et regarder la taille du fichier généré\noptimized_tflite_model = optimized_converter.convert()\nwith open('optimized_model.tflite', 'wb') as f:\n    f.write(optimized_tflite_model)\n\noptimized_tflite_file_size = os.path.getsize('optimized_model.tflite')\nprint(f\"Taille du fichier modèle TensorFlow Lite optimisé: {optimized_tflite_file_size} octets\")\n","metadata":{"tags":[],"cell_id":"454afc5bba8346b8ae51477e6995713f","source_hash":"c5ecd30d","execution_start":1679833966444,"execution_millis":991,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"INFO:tensorflow:Assets written to: /tmp/tmpg5ay08a4/assets\nINFO:tensorflow:Assets written to: /tmp/tmpg5ay08a4/assets\nTaille du fichier modèle TensorFlow Lite optimisé: 273064 octets\n2023-03-26 12:32:47.217262: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2023-03-26 12:32:47.217306: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2023-03-26 12:32:47.217458: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpg5ay08a4\n2023-03-26 12:32:47.218483: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2023-03-26 12:32:47.218509: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpg5ay08a4\n2023-03-26 12:32:47.222556: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2023-03-26 12:32:47.249066: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpg5ay08a4\n2023-03-26 12:32:47.256083: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 38625 microseconds.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Quelle type  de quantization Optimize.Default, utilise-t-elle ?\n","metadata":{"tags":[],"cell_id":"4805b73de8ce4a21a2e1fe91c9842af3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"77e520f11519466abbaf5d2fbce548ca","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Quantization aware training \n\nDans cette section on va s'intéresser à l'entraînement sensible à la quantification. L'idée est de simuler les effets de la quantification pendant l'entraînement pour que le modèle ajuste les poids afin de tenir ocmpte de la quantification. L'idée est de prendre un modèle déjà entraîné normalement et de le réentraîné en faisant un peu de quantization pendant l'entraînement. \n\n\nReprendre le modèle entraîné sur MNIST\n","metadata":{"tags":[],"cell_id":"367d08b829c24fd38368e6f8d8c965fb","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"model = tf.keras.models.load_model('model_relu.h5')\nmodel.summary()","metadata":{"tags":[],"cell_id":"274f633385c240a187157893e4ba1c1a","source_hash":"b8268fba","execution_start":1679834087458,"execution_millis":93,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 300)               235500    \n                                                                 \n dense_1 (Dense)             (None, 100)               30100     \n                                                                 \n dense_2 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"A l'aide de la fonction quantize de tensorflow_model_optimization, créer une seconde version de votre modèle entraîné nommé qat_model","metadata":{"tags":[],"cell_id":"423c7a65ef2c46a095b6c5fe5569f1d6","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"!pip install tensorflow_model_optimization==0.7.3","metadata":{"cell_id":"d9d90f3a53184d90ad7a3d53725d0015","source_hash":"9c388d65","execution_start":1679834157017,"execution_millis":5077,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Collecting tensorflow_model_optimization==0.7.3\n  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dm-tree~=0.1.1\n  Downloading dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 KB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six~=1.10 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from tensorflow_model_optimization==0.7.3) (1.16.0)\nRequirement already satisfied: numpy~=1.14 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from tensorflow_model_optimization==0.7.3) (1.21.6)\nInstalling collected packages: dm-tree, tensorflow_model_optimization\nSuccessfully installed dm-tree-0.1.8 tensorflow_model_optimization-0.7.3\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import tensorflow_model_optimization as tfmot\n\nqat_model = tfmot.quantization.keras.quantize_model(model)\nqat_model.summary()","metadata":{"cell_id":"87bb6e787217404ba1708fd8eb7dbcfd","source_hash":"b2d93e0","execution_start":1679834165487,"execution_millis":2183,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n quantize_layer (QuantizeLay  (None, 784)              3         \n er)                                                             \n                                                                 \n quant_dense (QuantizeWrappe  (None, 300)              235505    \n rV2)                                                            \n                                                                 \n quant_dense_1 (QuantizeWrap  (None, 100)              30105     \n perV2)                                                          \n                                                                 \n quant_dense_2 (QuantizeWrap  (None, 10)               1015      \n perV2)                                                          \n                                                                 \n=================================================================\nTotal params: 266,628\nTrainable params: 266,610\nNon-trainable params: 18\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"83b712c1310147d5bb4f255c5be226d0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Compiler le modèle","metadata":{"tags":[],"cell_id":"96e146c83f7b49498a364110ea3c668b","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"qat_model.compile(optimizer=SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"cell_id":"b795faad4aa4436b91d75ad441217d13","source_hash":"ca419d26","execution_start":1679834221933,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"cf21467d1e624ec69ff8c5bdca375222","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Afficher le summary du modèle. D'après vous ce modèle est-il quantifié ? ","metadata":{"tags":[],"cell_id":"436ac63912dd4ee390bbd75326ead950","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"qat_model.summary()","metadata":{"cell_id":"ff277dbe551b45cab791b1e7869b081a","source_hash":"9c089943","execution_start":1679834243247,"execution_millis":47,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n quantize_layer (QuantizeLay  (None, 784)              3         \n er)                                                             \n                                                                 \n quant_dense (QuantizeWrappe  (None, 300)              235505    \n rV2)                                                            \n                                                                 \n quant_dense_1 (QuantizeWrap  (None, 100)              30105     \n perV2)                                                          \n                                                                 \n quant_dense_2 (QuantizeWrap  (None, 10)               1015      \n perV2)                                                          \n                                                                 \n=================================================================\nTotal params: 266,628\nTrainable params: 266,610\nNon-trainable params: 18\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"5d7ac8ef4ebd4b1f9d405b7ca46d5c73","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Réentraîner votre modèle sur un sous ensemble des données (sur une ou deux epochs) et afficher la performance sur le train et test set","metadata":{"tags":[],"cell_id":"43f29e46310342ed945c422d98e86224","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"#1 Réentraîner votre modèle sur un sous ensemble des données (sur une ou deux epochs)\nhistory_qat = qat_model.fit(x_train_normalized[:5000], y_train[:5000], epochs=2, validation_data=(x_test_normalized[:1000], y_test[:1000]))\n\n#2  afficher la performance sur le train et test set\ntrain_loss, train_acc = qat_model.evaluate(x_train_normalized[:5000], y_train[:5000], verbose=2)\nprint(f\"Performance sur le train set - Loss: {train_loss}, Accuracy: {train_acc}\")\n\ntest_loss, test_acc = qat_model.evaluate(x_test_normalized[:1000], y_test[:1000], verbose=2)\nprint(f\"Performance sur le test set - Loss: {test_loss}, Accuracy: {test_acc}\")","metadata":{"cell_id":"7ea40fb9ec2d44ff8d4a8500aca7b0d1","source_hash":"f3f3f7a9","execution_start":1679834321757,"execution_millis":6576,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/2\n157/157 [==============================] - 2s 11ms/step - loss: 0.1329 - accuracy: 0.9668 - val_loss: 0.1209 - val_accuracy: 0.9710\nEpoch 2/2\n157/157 [==============================] - 2s 10ms/step - loss: 0.0823 - accuracy: 0.9818 - val_loss: 0.1128 - val_accuracy: 0.9710\n157/157 - 0s - loss: 0.0728 - accuracy: 0.9828 - 476ms/epoch - 3ms/step\nPerformance sur le train set - Loss: 0.07283902913331985, Accuracy: 0.9828000068664551\n32/32 - 0s - loss: 0.1128 - accuracy: 0.9710 - 109ms/epoch - 3ms/step\nPerformance sur le test set - Loss: 0.11279353499412537, Accuracy: 0.9710000157356262\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"54380bc8071544bfbf52ba3c92659127","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Convertir votre modèle avec TFLite","metadata":{"tags":[],"cell_id":"4fc1a831621d4068a21338f3660ae82f","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\ntflite_model = converter.convert()\n\nwith open('qat_model.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"tags":[],"cell_id":"2707fc3cf3c14604a3a4506b491fa444","source_hash":"e28bf47c","execution_start":1679834368863,"execution_millis":2684,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /tmp/tmpp_fz_jwq/assets\nINFO:tensorflow:Assets written to: /tmp/tmpp_fz_jwq/assets\n2023-03-26 12:39:31.232708: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2023-03-26 12:39:31.232760: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2023-03-26 12:39:31.232929: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpp_fz_jwq\n2023-03-26 12:39:31.235432: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2023-03-26 12:39:31.235469: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpp_fz_jwq\n2023-03-26 12:39:31.251886: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2023-03-26 12:39:31.308266: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpp_fz_jwq\n2023-03-26 12:39:31.322941: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 90011 microseconds.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Sauvegarder le modèle QAT et comparer les tailles des modèles","metadata":{"tags":[],"cell_id":"ba82c25b8a3c4154875def3d8e40a2f2","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"qat_model.save('qat_model.h5')\n\nqat_model_file_size = os.path.getsize('qat_model.h5')\nprint(f\"Taille du fichier modèle quantifié: {qat_model_file_size} octets\")\n","metadata":{"tags":[],"cell_id":"bf0068db56a943d695936935a5c3f00c","source_hash":"7cd22fa6","execution_start":1679834403136,"execution_millis":43,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Taille du fichier modèle quantifié: 1100392 octets\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Comparer les performances des trois modèles suivants (taille et accuracy) : \n- modèle original\n- modèle quantifié avec la post training quantization\n- modèle entraîné avec la training aware quantization\n\n\n","metadata":{"tags":[],"cell_id":"c8a053bcc814490c9a755c50f4a97557","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"fbedaca920ba41659068105b83c775e4","source_hash":"b623e53d","execution_start":1679833681857,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"eac99e55a4b641d7abc31ada23fa0f96","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3cbfad71-571a-4cfb-8f30-25f77afca130' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{"is_reactive":false},"orig_nbformat":2,"deepnote_notebook_id":"4bf2de564c474832b5da81f3b043c5ae","deepnote_execution_queue":[]}}